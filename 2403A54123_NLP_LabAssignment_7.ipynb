{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtCzZoY9cYYtPp+BNcfYg5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2403a54123-web/NLP/blob/main/2403A54123_NLP_LabAssignment_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv('/content/Dataset.csv')"
      ],
      "metadata": {
        "id": "cUuqu_L_4ZW4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUS5NOy36LcQ",
        "outputId": "15063151-4c64-486e-c9ef-4ab0b2ef3799"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      Football is popular worldwide.  Unnamed: 1\n",
            "0                Basketball players train very hard.         NaN\n",
            "1                 The Olympics unite many countries.         NaN\n",
            "2                Athletes need strong mental health.         NaN\n",
            "3      eSports competitions attract young audiences.         NaN\n",
            "4            Children learn teamwork through sports.         NaN\n",
            "5                  Elections allow citizens to vote.         NaN\n",
            "6               Governments create laws for society.         NaN\n",
            "7               Climate change affects many nations.         NaN\n",
            "8          People debate immigration policies often.         NaN\n",
            "9           Democracy requires active participation.         NaN\n",
            "10       Leaders make important political decisions.         NaN\n",
            "11  Doctors promote preventive healthcare practices.         NaN\n",
            "12        Eating vegetables improves overall health.         NaN\n",
            "13                   Exercise keeps the body strong.         NaN\n",
            "14                Vaccines prevent serious diseases.         NaN\n",
            "15                  Mental health is very important.         NaN\n",
            "16         Hospitals provide emergency medical care.         NaN\n",
            "17              Technology changes daily human life.         NaN\n",
            "18       Computers process information very quickly.         NaN\n",
            "19   Artificial intelligence improves many services.         NaN\n",
            "20      Cybersecurity protects online personal data.         NaN\n",
            "21              Smartphones connect people globally.         NaN\n",
            "22            Renewable energy supports clean power.         NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # Added to resolve LookupError\n",
        "\n",
        "documents = [\n",
        "\"Football is popular worldwide.\",\n",
        "\"Basketball players train very hard.\",\n",
        "\"The Olympics unite many countries.\",\n",
        "\"Athletes need strong mental health.\",\n",
        "\"eSports competitions attract young audiences.\",\n",
        "\"Children learn teamwork through sports.\",\n",
        "\"Elections allow citizens to vote.\",\n",
        "\"Governments create laws for society.\",\n",
        "\"Climate change affects many nations.\",\n",
        "\"People debate immigration policies often.\",\n",
        "\"Democracy requires active participation.\",\n",
        "\"Leaders make important political decisions.\",\n",
        "\"Doctors promote preventive healthcare practices.\",\n",
        "\"Eating vegetables improves overall health.\",\n",
        "\"Exercise keeps the body strong.\",\n",
        "\"Vaccines prevent serious diseases.\",\n",
        "\"Mental health is very important.\",\n",
        "\"Hospitals provide emergency medical care.\",\n",
        "\"Technology changes daily human life.\",\n",
        "\"Computers process information very quickly.\",\n",
        "\"Artificial intelligence improves many services.\",\n",
        "\"Cybersecurity protects online personal data.\",\n",
        "\"Smartphones connect people globally.\",\n",
        "\"Renewable energy supports clean power.\"\n",
        "]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "cleaned_documents = [preprocess(doc) for doc in documents]\n",
        "\n",
        "print(cleaned_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQh6drKy8JXX",
        "outputId": "d1a96eea-46e7-4408-efac-bca0a8a101c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['football popular worldwide', 'basketball player train hard', 'olympics unite many country', 'athlete need strong mental health', 'esports competition attract young audience', 'child learn teamwork sport', 'election allow citizen vote', 'government create law society', 'climate change affect many nation', 'people debate immigration policy often', 'democracy requires active participation', 'leader make important political decision', 'doctor promote preventive healthcare practice', 'eating vegetable improves overall health', 'exercise keep body strong', 'vaccine prevent serious disease', 'mental health important', 'hospital provide emergency medical care', 'technology change daily human life', 'computer process information quickly', 'artificial intelligence improves many service', 'cybersecurity protects online personal data', 'smartphones connect people globally', 'renewable energy support clean power']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documents = [\n",
        "    \"Football is popular worldwide\",\n",
        "    \"Artificial intelligence improves many services\",\n",
        "    \"Doctors promote preventive healthcare practices\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Feature Names:\\n\", vectorizer.get_feature_names_out())\n",
        "print(\"\\nTF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-yMrYM665dn",
        "outputId": "4e107d0d-0eef-4214-c344-312fd4027600"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names:\n",
            " ['artificial' 'doctors' 'football' 'healthcare' 'improves' 'intelligence'\n",
            " 'is' 'many' 'popular' 'practices' 'preventive' 'promote' 'services'\n",
            " 'worldwide']\n",
            "\n",
            "TF-IDF Matrix:\n",
            " [[0.        0.        0.5       0.        0.        0.        0.5\n",
            "  0.        0.5       0.        0.        0.        0.        0.5      ]\n",
            " [0.4472136 0.        0.        0.        0.4472136 0.4472136 0.\n",
            "  0.4472136 0.        0.        0.        0.        0.4472136 0.       ]\n",
            " [0.        0.4472136 0.        0.4472136 0.        0.        0.\n",
            "  0.        0.        0.4472136 0.4472136 0.4472136 0.        0.       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Example documents (can use your full dataset)\n",
        "documents = [\n",
        "    \"Football is popular worldwide\",\n",
        "    \"Basketball players train very hard\",\n",
        "    \"Doctors promote preventive healthcare practices\",\n",
        "    \"Artificial intelligence improves many services\",\n",
        "    \"Exercise keeps the body strong\"\n",
        "]\n",
        "\n",
        "# Convert to TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# Print similarity scores\n",
        "print(\"Cosine Similarity Matrix:\\n\")\n",
        "print(np.round(similarity_matrix, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDZB32O57Fen",
        "outputId": "af1aa57f-48e5-4ac5-c223-f1b8b77470b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Matrix:\n",
            "\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 5 example documents\n",
        "documents = [\n",
        "    \"Football is popular worldwide.\",\n",
        "    \"Athletes need strong mental health.\",\n",
        "    \"Doctors promote preventive healthcare practices.\",\n",
        "    \"Technology changes daily human life.\",\n",
        "    \"Artificial intelligence improves many services.\"\n",
        "]\n",
        "\n",
        "# Simple document names\n",
        "doc_names = [\"D1\", \"D2\", \"D3\", \"D4\", \"D5\"]\n",
        "\n",
        "# Preprocess: lowercase + remove punctuation\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "cleaned_docs = [preprocess(doc) for doc in documents]\n",
        "\n",
        "# Convert to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(cleaned_docs)\n",
        "\n",
        "# Compute Cosine Similarity\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# Convert to DataFrame with document names\n",
        "similarity_df = pd.DataFrame(similarity_matrix, index=doc_names, columns=doc_names)\n",
        "\n",
        "# Print similarity matrix\n",
        "print(\"Cosine Similarity Matrix:\\n\")\n",
        "print(similarity_df.round(2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD1LfttC9Uf3",
        "outputId": "362662ee-1f76-452e-87e5-d2917aa95933"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Matrix:\n",
            "\n",
            "     D1   D2   D3   D4   D5\n",
            "D1  1.0  0.0  0.0  0.0  0.0\n",
            "D2  0.0  1.0  0.0  0.0  0.0\n",
            "D3  0.0  0.0  1.0  0.0  0.0\n",
            "D4  0.0  0.0  0.0  1.0  0.0\n",
            "D5  0.0  0.0  0.0  0.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# 5 example documents\n",
        "documents = [\n",
        "    \"Football is popular worldwide.\",\n",
        "    \"Athletes need strong mental health.\",\n",
        "    \"Doctors promote preventive healthcare practices.\",\n",
        "    \"Technology changes daily human life.\",\n",
        "    \"Artificial intelligence improves many services.\"\n",
        "]\n",
        "\n",
        "doc_names = [\"D1\", \"D2\", \"D3\", \"D4\", \"D5\"]\n",
        "\n",
        "# Preprocess: lowercase + remove punctuation\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "cleaned_docs = [preprocess(doc) for doc in documents]\n",
        "\n",
        "# --- Cosine Similarity (for comparison) ---\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(cleaned_docs)\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "cosine_df = pd.DataFrame(cosine_sim, index=doc_names, columns=doc_names)\n",
        "\n",
        "# --- Jaccard Similarity ---\n",
        "def jaccard_similarity(doc1, doc2):\n",
        "    set1 = set(doc1.split())\n",
        "    set2 = set(doc2.split())\n",
        "    return len(set1 & set2) / len(set1 | set2)\n",
        "\n",
        "# Compute Jaccard similarity matrix\n",
        "jaccard_matrix = [[jaccard_similarity(d1, d2) for d2 in cleaned_docs] for d1 in cleaned_docs]\n",
        "jaccard_df = pd.DataFrame(jaccard_matrix, index=doc_names, columns=doc_names)\n",
        "\n",
        "# Print both matrices\n",
        "print(\"Cosine Similarity Matrix:\\n\")\n",
        "print(cosine_df.round(2))\n",
        "print(\"\\nJaccard Similarity Matrix:\\n\")\n",
        "print(jaccard_df.round(2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAL1Ep5O9_Du",
        "outputId": "0a7345eb-f808-42d3-9c6b-f5c68d090ada"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Matrix:\n",
            "\n",
            "     D1   D2   D3   D4   D5\n",
            "D1  1.0  0.0  0.0  0.0  0.0\n",
            "D2  0.0  1.0  0.0  0.0  0.0\n",
            "D3  0.0  0.0  1.0  0.0  0.0\n",
            "D4  0.0  0.0  0.0  1.0  0.0\n",
            "D5  0.0  0.0  0.0  0.0  1.0\n",
            "\n",
            "Jaccard Similarity Matrix:\n",
            "\n",
            "     D1   D2   D3   D4   D5\n",
            "D1  1.0  0.0  0.0  0.0  0.0\n",
            "D2  0.0  1.0  0.0  0.0  0.0\n",
            "D3  0.0  0.0  1.0  0.0  0.0\n",
            "D4  0.0  0.0  0.0  1.0  0.0\n",
            "D5  0.0  0.0  0.0  0.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "import re\n",
        "\n",
        "# Download WordNet if not already\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 10 documents\n",
        "docs = [\n",
        "    \"Football is popular worldwide.\",\n",
        "    \"Athletes need strong mental health.\",\n",
        "    \"Doctors promote preventive healthcare practices.\",\n",
        "    \"Eating vegetables improves overall health.\",\n",
        "    \"Exercise keeps the body strong.\",\n",
        "    \"Technology changes daily human life.\",\n",
        "    \"Computers process information very quickly.\",\n",
        "    \"Artificial intelligence improves many services.\",\n",
        "    \"Smartphones connect people globally.\",\n",
        "    \"Renewable energy supports clean power.\"\n",
        "]\n",
        "\n",
        "doc_names = [f\"D{i}\" for i in range(1, 11)]\n",
        "\n",
        "# Preprocess: lowercase + remove punctuation\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "cleaned_docs = [preprocess(doc) for doc in docs]\n",
        "\n",
        "# Function to compute average Wu-Palmer similarity between two sentences\n",
        "def sentence_similarity(sent1, sent2):\n",
        "    words1 = sent1.split()\n",
        "    words2 = sent2.split()\n",
        "    sims = []\n",
        "    for w1 in words1:\n",
        "        # Ensure we only consider nouns for WordNet similarity if pos=wn.NOUN is used\n",
        "        # Or remove pos=wn.NOUN to allow other parts of speech if desired\n",
        "        syns1 = wn.synsets(w1, pos=wn.NOUN)\n",
        "        if not syns1:\n",
        "            continue\n",
        "        for w2 in words2:\n",
        "            syns2 = wn.synsets(w2, pos=wn.NOUN)\n",
        "            if not syns2:\n",
        "                continue\n",
        "            # Get the first synset for simplicity; more advanced approaches might try all combinations\n",
        "            sim = syns1[0].wup_similarity(syns2[0])\n",
        "            if sim:\n",
        "                sims.append(sim)\n",
        "    if sims:\n",
        "        return sum(sims)/len(sims)\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Example 10 sentence pairs defined by their indices in cleaned_docs\n",
        "sentence_pairs_with_indices = [\n",
        "    (0, 1),  # D1 ↔ D2\n",
        "    (1, 2),  # D2 ↔ D3\n",
        "    (2, 3),  # D3 ↔ D4\n",
        "    (3, 4),  # D4 ↔ D5\n",
        "    (4, 1),  # D5 ↔ D2\n",
        "    (5, 6),  # D6 ↔ D7\n",
        "    (6, 7),  # D7 ↔ D8\n",
        "    (7, 8),  # D8 ↔ D9\n",
        "    (8, 5),  # D9 ↔ D6\n",
        "    (9, 5)   # D10 ↔ D6\n",
        "]\n",
        "\n",
        "# Compute semantic similarity\n",
        "for idx1, idx2 in sentence_pairs_with_indices:\n",
        "    s1 = cleaned_docs[idx1]\n",
        "    s2 = cleaned_docs[idx2]\n",
        "    d1 = doc_names[idx1]\n",
        "    d2 = doc_names[idx2]\n",
        "    sim = sentence_similarity(s1, s2)\n",
        "    print(f\"{d1} ↔ {d2} | Average Wu-Palmer Similarity: {sim:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OItZdvFd_JS_",
        "outputId": "237d4122-c60b-424d-c32e-a0da229049bb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D1 ↔ D2 | Average Wu-Palmer Similarity: 0.19\n",
            "D2 ↔ D3 | Average Wu-Palmer Similarity: 0.23\n",
            "D3 ↔ D4 | Average Wu-Palmer Similarity: 0.20\n",
            "D4 ↔ D5 | Average Wu-Palmer Similarity: 0.21\n",
            "D5 ↔ D2 | Average Wu-Palmer Similarity: 0.23\n",
            "D6 ↔ D7 | Average Wu-Palmer Similarity: 0.30\n",
            "D7 ↔ D8 | Average Wu-Palmer Similarity: 0.35\n",
            "D8 ↔ D9 | Average Wu-Palmer Similarity: 0.35\n",
            "D9 ↔ D6 | Average Wu-Palmer Similarity: 0.27\n",
            "D10 ↔ D6 | Average Wu-Palmer Similarity: 0.27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Ensure WordNet is downloaded\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# Using the cleaned_docs and doc_names from previous cells for consistency\n",
        "# cleaned_docs was last updated in cell OItZdvFd_JS_\n",
        "# doc_names was last updated in cell OItZdvFd_JS_\n",
        "\n",
        "print(\"Comparing Cosine, Jaccard, and WordNet-based Similarities:\\n\")\n",
        "\n",
        "# --- 1. Cosine Similarity (TF-IDF based) ---\n",
        "print(\"--- Cosine Similarity Matrix (TF-IDF based) ---\")\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(cleaned_docs)\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "cosine_df = pd.DataFrame(cosine_sim_matrix, index=doc_names, columns=doc_names)\n",
        "print(cosine_df.round(2))\n",
        "print(\"\\n\")\n",
        "\n",
        "# --- 2. Jaccard Similarity ---\n",
        "print(\"--- Jaccard Similarity Matrix ---\")\n",
        "def jaccard_similarity(doc1, doc2):\n",
        "    set1 = set(doc1.split())\n",
        "    set2 = set(doc2.split())\n",
        "    if not (set1 | set2): # Handle case where both sets are empty\n",
        "        return 0.0\n",
        "    return len(set1 & set2) / len(set1 | set2)\n",
        "\n",
        "jaccard_matrix = [[jaccard_similarity(d1, d2) for d2 in cleaned_docs] for d1 in cleaned_docs]\n",
        "jaccard_df = pd.DataFrame(jaccard_matrix, index=doc_names, columns=doc_names)\n",
        "print(jaccard_df.round(2))\n",
        "print(\"\\n\")\n",
        "\n",
        "# --- 3. WordNet-based Similarity (Wu-Palmer) ---\n",
        "print(\"--- WordNet-based Wu-Palmer Similarity for selected pairs ---\")\n",
        "\n",
        "def sentence_similarity_wup(sent1, sent2):\n",
        "    words1 = sent1.split()\n",
        "    words2 = sent2.split()\n",
        "    sims = []\n",
        "    for w1 in words1:\n",
        "        syns1 = wn.synsets(w1, pos=wn.NOUN)\n",
        "        if not syns1:\n",
        "            continue\n",
        "        for w2 in words2:\n",
        "            syns2 = wn.synsets(w2, pos=wn.NOUN)\n",
        "            if not syns2:\n",
        "                continue\n",
        "            sim = syns1[0].wup_similarity(syns2[0])\n",
        "            if sim:\n",
        "                sims.append(sim)\n",
        "    if sims:\n",
        "        return sum(sims)/len(sims)\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Using the same sentence pairs as in cell OItZdvFd_JS_ for comparison\n",
        "sentence_pairs_with_indices = [\n",
        "    (0, 1),  # D1 ↔ D2\n",
        "    (1, 2),  # D2 ↔ D3\n",
        "    (2, 3),  # D3 ↔ D4\n",
        "    (3, 4),  # D4 ↔ D5\n",
        "    (4, 1),  # D5 ↔ D2\n",
        "    (5, 6),  # D6 ↔ D7\n",
        "    (6, 7),  # D7 ↔ D8\n",
        "    (7, 8),  # D8 ↔ D9\n",
        "    (8, 5),  # D9 ↔ D6\n",
        "    (9, 5)   # D10 ↔ D6\n",
        "]\n",
        "\n",
        "for idx1, idx2 in sentence_pairs_with_indices:\n",
        "    s1 = cleaned_docs[idx1]\n",
        "    s2 = cleaned_docs[idx2]\n",
        "    d1 = doc_names[idx1]\n",
        "    d2 = doc_names[idx2]\n",
        "    sim = sentence_similarity_wup(s1, s2)\n",
        "    print(f\"{d1} ↔ {d2} | Average Wu-Palmer Similarity: {sim:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCKeiaExWqLP",
        "outputId": "ed328b99-50a4-4d87-9b92-dc81ef740ec4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Cosine, Jaccard, and WordNet-based Similarities:\n",
            "\n",
            "--- Cosine Similarity Matrix (TF-IDF based) ---\n",
            "      D1    D2   D3    D4    D5   D6   D7    D8   D9  D10\n",
            "D1   1.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.0  0.0\n",
            "D2   0.0  1.00  0.0  0.16  0.18  0.0  0.0  0.00  0.0  0.0\n",
            "D3   0.0  0.00  1.0  0.00  0.00  0.0  0.0  0.00  0.0  0.0\n",
            "D4   0.0  0.16  0.0  1.00  0.00  0.0  0.0  0.18  0.0  0.0\n",
            "D5   0.0  0.18  0.0  0.00  1.00  0.0  0.0  0.00  0.0  0.0\n",
            "D6   0.0  0.00  0.0  0.00  0.00  1.0  0.0  0.00  0.0  0.0\n",
            "D7   0.0  0.00  0.0  0.00  0.00  0.0  1.0  0.00  0.0  0.0\n",
            "D8   0.0  0.00  0.0  0.18  0.00  0.0  0.0  1.00  0.0  0.0\n",
            "D9   0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  1.0  0.0\n",
            "D10  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.0  1.0\n",
            "\n",
            "\n",
            "--- Jaccard Similarity Matrix ---\n",
            "      D1    D2   D3    D4    D5   D6   D7    D8   D9  D10\n",
            "D1   1.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.0  0.0\n",
            "D2   0.0  1.00  0.0  0.11  0.11  0.0  0.0  0.00  0.0  0.0\n",
            "D3   0.0  0.00  1.0  0.00  0.00  0.0  0.0  0.00  0.0  0.0\n",
            "D4   0.0  0.11  0.0  1.00  0.00  0.0  0.0  0.11  0.0  0.0\n",
            "D5   0.0  0.11  0.0  0.00  1.00  0.0  0.0  0.00  0.0  0.0\n",
            "D6   0.0  0.00  0.0  0.00  0.00  1.0  0.0  0.00  0.0  0.0\n",
            "D7   0.0  0.00  0.0  0.00  0.00  0.0  1.0  0.00  0.0  0.0\n",
            "D8   0.0  0.00  0.0  0.11  0.00  0.0  0.0  1.00  0.0  0.0\n",
            "D9   0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  1.0  0.0\n",
            "D10  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.0  1.0\n",
            "\n",
            "\n",
            "--- WordNet-based Wu-Palmer Similarity for selected pairs ---\n",
            "D1 ↔ D2 | Average Wu-Palmer Similarity: 0.19\n",
            "D2 ↔ D3 | Average Wu-Palmer Similarity: 0.23\n",
            "D3 ↔ D4 | Average Wu-Palmer Similarity: 0.20\n",
            "D4 ↔ D5 | Average Wu-Palmer Similarity: 0.21\n",
            "D5 ↔ D2 | Average Wu-Palmer Similarity: 0.23\n",
            "D6 ↔ D7 | Average Wu-Palmer Similarity: 0.30\n",
            "D7 ↔ D8 | Average Wu-Palmer Similarity: 0.35\n",
            "D8 ↔ D9 | Average Wu-Palmer Similarity: 0.35\n",
            "D9 ↔ D6 | Average Wu-Palmer Similarity: 0.27\n",
            "D10 ↔ D6 | Average Wu-Palmer Similarity: 0.27\n"
          ]
        }
      ]
    }
  ]
}